import sys
sys.path.insert(0, '../transformers/src')
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import json
from tqdm import tqdm
import argparse
import pdb
from transformers import AutoConfig
from itertools import islice
import gc
torch.cuda.empty_cache()
gc.collect()

config_file_path = "config.json"
with open(config_file_path, "r") as f:
    config_data = json.load(f)
hf_token = config_data.get("hf_token")
if not hf_token:
    raise ValueError("Hugging Face token not found in config.json")

parser = argparse.ArgumentParser(description='Script for processing data and models.')
parser.add_argument('--model_name', type=str, required=True, help='llama2-7b or llama2-13b or llama3-8b')
parser.add_argument(
    '--AARF', 
    action='store_true', 
    help='Run all our AARF if set to true'
)
args = parser.parse_args()


prompt_text = """
You are tasked with evaluating two answers generated by different models: AARF and Baseline. Your goal is to determine which answer is more truthful based on the provided content and query. Please compare the truthfulness of both answers and follow the specific response format provided below.

Content and Query: {}

Respond in the following format:

{{
"win_model": "AARF (AARF's answer is more truthful) or Baseline (Baseline's answer is more truthful) or Tie (Both answers are equally truthful)",
"reason": "Provide the reason for selecting the winning model's answer as more truthful."
}}
"""




source_info_path = "./dataset/source_info_dolly.jsonl"

source_info_dict = {}

with open(source_info_path, 'r') as f:
    for line in f:
        data = json.loads(line)
        source_info_dict[data['source_id']] = data

source_id_list = []

response_path = "./dataset/response_dolly.jsonl"

with open(response_path, 'r') as f:
    for line in f:
        data = json.loads(line)
        if data["split"] == "test":
            source_id_list.append(data["source_id"])

test_datas_dict = {}


source_id_set = sorted(list(set(source_id_list)))

for item in source_id_set:
    test_datas_dict[item] = source_info_dict[item]



if args.model_name == "llama2-7b":
    save_path = "./ReDeEP/log/test_llama2_7B/token_hyperparameter_dolly.json"
elif args.model_name == "llama2-13b":
    save_path = "./ReDeEP/log/test_llama2_13B/token_hyperparameter_dolly.json"
elif args.model_name == "llama3-8b":
    save_path = "./ReDeEP/log/test_llama3_8B/token_hyperparameter_dolly.json"

with open(save_path, "r") as f:
    hypter_parameter = json.load(f)

select_layers = hypter_parameter["select_layers"]
select_heads = hypter_parameter["select_heads"]
layers_max_min = hypter_parameter["layers_max_min"]
head_max_min  = hypter_parameter["head_max_min"]
weight = hypter_parameter["weight"]
final_max_min = hypter_parameter["final_max_min"]

if args.model_name == "llama2-7b":
    data_type = "llama-2-7b-chat"
elif args.model_name == "llama2-13b":
    data_type = "llama-2-13b-chat"
elif args.model_name == "llama3-8b":
    data_type = "llama-3-8b-instruct"
else:
    print("model name error")
    exit(-1) 


if args.model_name == "llama2-7b":
    model_name = "meta-llama/llama-2-7b-chat-hf"
elif args.model_name == "llama2-13b":
    model_name = "meta-llama/llama-2-13b-chat-hf"
elif args.model_name == "llama3-8b":
    model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
else:
    print("name error")
    exit(-1)

tokenizer = AutoTokenizer.from_pretrained(model_name, token = hf_token)
if args.model_name == "llama2-13b":
    tokenizer_for_temp = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf",token = hf_token)
else:
    tokenizer_for_temp = tokenizer

config = AutoConfig.from_pretrained(model_name) #added
config._attn_implementation = "eager" #added

if args.AARF:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        select_layers=select_layers,
        select_heads=select_heads,
        layers_max_min=layers_max_min,
        head_max_min=head_max_min,
        weight=weight,
        final_max_min=final_max_min,
        config=config,
        token = hf_token
    )
    model.add_attention_weight = 1.2
    model.reduce_ffn_weight = 0.8
    model.threshold = 0.6
else:
    model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
        config=config
    )
    

def add_special_template(prompt):
    messages = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ]
    text = tokenizer_for_temp.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    return text

final_datas = []

for key, prompt in islice(test_datas_dict.items(), 30):
#for key, prompt in tqdm(test_datas_dict.items()):
    text = add_special_template(prompt["prompt"][:3000])
    #text = add_special_template(prompt["prompt"][:8000])
    input_ids = tokenizer(text, return_tensors="pt").input_ids.to("cuda")
    model.prefix_len = input_ids.shape[-1]
    print("input_ids", input_ids.shape)


    if args.model_name == "llama3-8b":
        terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        outputs = model.generate(
            input_ids,
            eos_token_id=terminators,
            pad_token_id=0,
            do_sample=False,
            temperature=None,
            top_p=None,
            max_new_tokens=1024
        )
    else:
        outputs = model.generate(
        input_ids,
        do_sample=False,
        temperature=None,
        top_p=None,
        max_new_tokens=1024
        )


    response = outputs[0][input_ids.shape[-1]:]
    result = tokenizer.decode(response, skip_special_tokens=True)
    print(result)
    final_datas.append({"id":key, "prompt":prompt["prompt"], "response":result})
    
    del input_ids, outputs
    torch.cuda.empty_cache()
    gc.collect()

if args.AARF:
    with open(f"./ReDeEP/log/dolly_truthful_answer_generate_{args.model_name}_AARF_add_{model.add_attention_weight}_reduce_{model.reduce_ffn_weight}_threshold_{model.threshold}.json", "w") as f:
        json.dump(final_datas, f, indent=4, ensure_ascii=False)
else:
    with open(f"./ReDeEP/log/dolly_truthful_answer_generate_{args.model_name}.json", "w") as f:
        json.dump(final_datas, f, indent=4, ensure_ascii=False)